{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPlq22ZlLW4wVe5AtvcVgvA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tashidu/ChatBox-Langchain/blob/main/Langchain_Chatbox.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simple Chatbot using LangChain and OpenAI"
      ],
      "metadata": {
        "id": "BwThHufFge-t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6wVePEAPa0n2",
        "outputId": "22986fd9-2d1d-412f-bc41-1ada3cdb21a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.27)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.72)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.9)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.4.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.14.1)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (3.11.0)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.7.14)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-openai -qU"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ITmB6mTb3bL",
        "outputId": "d36bdc14-08bd-4a41-be42-77c863f5bfa0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/70.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.6/70.6 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "Z4zmORfIcC4U"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize OpenAi LLM"
      ],
      "metadata": {
        "id": "iCkpObiQgjok"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)"
      ],
      "metadata": {
        "id": "EAwRarsvcNq6"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Promot Template"
      ],
      "metadata": {
        "id": "QhgJPnpXgrEV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages (\n",
        "    [\n",
        "        (\"system\", \"You are an intelligent chatbot , answer flollowing question\"),\n",
        "        (\"user\", \"{question}\")\n",
        "    ]\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "nSKdRXGGcwAr"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "output parser"
      ],
      "metadata": {
        "id": "HDKtIwkXgvjt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Initialize the string output parser\n",
        "parser = StrOutputParser()"
      ],
      "metadata": {
        "id": "D7gnEZY4djoZ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Chain the prompt, LLM, and output parser\n",
        "chain = prompt | llm | parser"
      ],
      "metadata": {
        "id": "bdEUlxSaiU1r"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"My name is codeprolk\"\n",
        "\n",
        "response = chain.invoke({\"question\": question})\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCw1sZ23iWBV",
        "outputId": "a204f5c1-e4c0-40a6-a043-040a83042602"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nice to meet you, codeprolk! How can I assist you today?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize Prompt Template for Dynamic Interaction"
      ],
      "metadata": {
        "id": "qAFENJmxpBFV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import MessagesPlaceholder\n",
        "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
        "\n",
        "# Create a prompt template using MessagesPlaceholder for the question\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        SystemMessage(content=\"You are an intelligent chatbot. Answer the following question.\"),\n",
        "        MessagesPlaceholder(variable_name=\"question\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Chain the prompt, LLM, and output parser\n",
        "chain = prompt | llm | parser\n",
        ""
      ],
      "metadata": {
        "id": "IzLBiItipBwL"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"My name is codeprolk\"\n",
        "\n",
        "response = chain.invoke({\"question\": [HumanMessage(content=question)]})\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWDXMcb4pI4z",
        "outputId": "a8c9b331-cb82-460e-c523-3fa9a5777bce"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nice to meet you, codeprolk! How can I assist you today?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "question = \"Who am I\"\n",
        "\n",
        "response = chain.invoke({\"question\": [HumanMessage(content=question)]})\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GhsH19gqpQhi",
        "outputId": "d581f7b0-238e-4aab-9547-a53dcee54e34"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are a user interacting with me, a chatbot, on this platform. How can I assist you today?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create a prompt template with a predefined conversation history and a new question placeholder\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        SystemMessage(content=\"You are an intelligent chatbot. Answer the following question.\"),\n",
        "        HumanMessage(content=\"My name is codeprolk\"),\n",
        "        AIMessage(content=\"Nice to meet you, codeprolk! How can I assist you today?\"),\n",
        "        MessagesPlaceholder(variable_name=\"question\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Chain the prompt, LLM, and output parser\n",
        "chain = prompt | llm | parser"
      ],
      "metadata": {
        "id": "2RCN7uWgpYhz"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Who am I\"\n",
        "\n",
        "response = chain.invoke({\"question\": [HumanMessage(content=question)]})\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHTQiBhZpbUr",
        "outputId": "7e6d1eef-6f14-4152-8124-e20d483a7560"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are codeprolk, a user interacting with me right now. How can I help you further, codeprolk?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize Prompt Template to Handle Dynamic Conversation History"
      ],
      "metadata": {
        "id": "ovG-9rDZpeL7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a prompt template with a dynamic conversation history and a new question placeholder\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        SystemMessage(content=\"You are an intelligent chatbot. Answer the following question.\"),\n",
        "        MessagesPlaceholder(variable_name=\"history\"),\n",
        "        MessagesPlaceholder(variable_name=\"question\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Chain the prompt, LLM, and output parser\n",
        "chain = prompt | llm | parser"
      ],
      "metadata": {
        "id": "e8n2U5aTpfC7"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the conversation history\n",
        "history = [\n",
        "    HumanMessage(content=\"My name is codeprolk\"),\n",
        "    AIMessage(content=\"Nice to meet you, codeprolk! How can I assist you today?\"),\n",
        "    HumanMessage(content=\"what is 2 + 2\"),\n",
        "    AIMessage(content=\"4\")\n",
        "]\n"
      ],
      "metadata": {
        "id": "ANa9aPHcpg4r"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "question = \"Who am I\"\n",
        "\n",
        "response = chain.invoke({\"history\": history, \"question\": [HumanMessage(content=question)]})\n",
        "\n",
        "print(response)\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8MV91kDpkGq",
        "outputId": "33024c42-ae12-4da6-aced-f8f62da3f12b"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are codeprolk.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GW0wPzbOpq6K",
        "outputId": "101f6bec-c819-409c-81af-a3cd01474dda"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content='My name is codeprolk', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Nice to meet you, codeprolk! How can I assist you today?', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='what is 2 + 2', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='4', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Extend the history with the latest question and response\n",
        "history.extend([HumanMessage(content=question), AIMessage(content=response)])"
      ],
      "metadata": {
        "id": "hDsucCBipr0K"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"what's my last question?\"\n",
        "\n",
        "response = chain.invoke({\"history\": history, \"question\": [HumanMessage(content=question)]})\n",
        "\n",
        "history.extend([HumanMessage(content=question), AIMessage(content=response)])\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S14L6LXZpt4L",
        "outputId": "580bdaab-fe68-4392-f2f6-1e6ba942cf38"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your last question was \"Who am I\".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the last four interactions in the conversation history\n",
        "history[-4:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ztQLErYbp3Vq",
        "outputId": "70024424-ad93-42c7-9bf5-464f1a0bcad4"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content='Who am I', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='You are codeprolk.', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content=\"what's my last question?\", additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Your last question was \"Who am I\".', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    }
  ]
}